{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c79872fc",
   "metadata": {},
   "source": [
    "# Pull data from World Bank WDI and save to local CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21508728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved WDI data to ../data/raw/economic_dev_2000_2023.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pull data from World Bank WDI and save to local CSV\n",
    "\n",
    "Dependencies:\n",
    "    pip install wbgapi pandas\n",
    "\"\"\"\n",
    "\n",
    "import wbgapi as wb\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def fetch_and_save_wdi(\n",
    "    indicators: list,\n",
    "    countries: list = None,\n",
    "    years: list = None,\n",
    "    output_dir: str = \"data/raw\",\n",
    "    output_filename: str = \"wdi_data.csv\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch World Bank WDI data and save as CSV.\n",
    "\n",
    "    Parameters:\n",
    "        indicators: List of indicator codes, e.g. [\"NY.GDP.PCAP.KD\", \"SL.EMP.TOTL.SP.ZS\", \"NY.GDP.MKTP.KD.ZG\"]\n",
    "        countries: List of country ISO-2/ISO-3 codes, default None means all available countries\n",
    "        years: List of year range, e.g. list(range(2000, 2024))\n",
    "        output_dir: Local save directory\n",
    "        output_filename: Output file name\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if countries is None:\n",
    "        countries = wb.economy.list()  # Get codes for all economies\n",
    "\n",
    "    # If years not specified, use 1960–latest\n",
    "    if years is None:\n",
    "        years = list(range(1960, pd.Timestamp.now().year + 1))\n",
    "\n",
    "    # Fetch data: DataFrame with row index (economy, year), columns as indicators\n",
    "    df = wb.data.DataFrame(\n",
    "        indicators,\n",
    "        economy=countries,\n",
    "        time=years,\n",
    "        labels=True\n",
    "    )\n",
    "\n",
    "    # Reset index, convert economy and time to regular columns\n",
    "    df = df.reset_index().rename(columns={\"economy\": \"country\", \"time\": \"year\"})\n",
    "\n",
    "    # Save as CSV (no index column)\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"Saved WDI data to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    indicators = [\n",
    "        \"NY.GDP.PCAP.KD\",\n",
    "        \"SL.EMP.TOTL.SP.ZS\",\n",
    "        \"NY.GDP.MKTP.KD.ZG\"\n",
    "    ]\n",
    "\n",
    "    # [\"USA\",\"CHN\",\"IND\"]\n",
    "    countries = [\"USA\", \"CHN\", \"IND\"]\n",
    "\n",
    "    # Specify year range\n",
    "    years = list(range(2000, 2024))\n",
    "\n",
    "    # Execute fetch and save\n",
    "    fetch_and_save_wdi(\n",
    "        indicators=indicators,\n",
    "        countries=countries,\n",
    "        years=years,\n",
    "        output_dir=\"../data/raw\",\n",
    "        output_filename=\"economic_dev_2000_2023.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb4c935",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40d3c6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean data saved to ../data/clean/wdi_long_clean.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Use DuckDB in Python to perform SQL cleaning/unpivoting of WDI wide table, and export long-format CSV.\n",
    "Dependencies:\n",
    "    pip install duckdb pandas\n",
    "\"\"\"\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "RAW_CSV        = \"../data/raw/economic_dev_2000_2023.csv\"\n",
    "OUTPUT_CSV     = \"../data/clean/wdi_long_clean.csv\"\n",
    "DB_FILE        = \"../data/tmp/wdi.duckdb\"\n",
    "\n",
    "os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(DB_FILE), exist_ok=True)\n",
    "\n",
    "# 1) Connect to DuckDB (file storage), or create in-memory database with \":memory:\"\n",
    "con = duckdb.connect(database=DB_FILE, read_only=False)\n",
    "\n",
    "# 2) Read CSV into a DuckDB table raw_widi, rename to avoid case conflicts in SELECT\n",
    "con.execute(f\"\"\"\n",
    "CREATE OR REPLACE TABLE raw_wdi AS\n",
    "SELECT\n",
    "    country        AS country_code,\n",
    "    series         AS indicator_code,\n",
    "    \"Country\"      AS country_name,\n",
    "    \"Series\"       AS indicator_name,\n",
    "    YR2000, YR2001, YR2002, YR2003, YR2004, YR2005,\n",
    "    YR2006, YR2007, YR2008, YR2009, YR2010, YR2011,\n",
    "    YR2012, YR2013, YR2014, YR2015, YR2016, YR2017,\n",
    "    YR2018, YR2019, YR2020, YR2021, YR2022, YR2023\n",
    "FROM read_csv_auto('{RAW_CSV}');\n",
    "\"\"\")\n",
    "\n",
    "# 3) Use SQL UNPIVOT to transform wide table to long table, filter out NULLs\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE clean_wdi AS\n",
    "SELECT\n",
    "    country_code,\n",
    "    indicator_code,\n",
    "    country_name,\n",
    "    indicator_name,\n",
    "    CAST(REPLACE(col, 'YR', '') AS INTEGER) AS year,\n",
    "    value\n",
    "FROM raw_wdi\n",
    "UNPIVOT (\n",
    "    value FOR col IN (\n",
    "        YR2000, YR2001, YR2002, YR2003, YR2004, YR2005,\n",
    "        YR2006, YR2007, YR2008, YR2009, YR2010, YR2011,\n",
    "        YR2012, YR2013, YR2014, YR2015, YR2016, YR2017,\n",
    "        YR2018, YR2019, YR2020, YR2021, YR2022, YR2023\n",
    "    )\n",
    ")\n",
    "WHERE value IS NOT NULL\n",
    "ORDER BY country_code, indicator_code, year;\n",
    "\"\"\")\n",
    "\n",
    "# 4) Export cleaned long table to local CSV\n",
    "df_clean = con.execute(\"SELECT * FROM clean_wdi\").df()\n",
    "df_clean.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "con.close()\n",
    "print(\"Clean data saved to\", OUTPUT_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf67ebe",
   "metadata": {},
   "source": [
    "# Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c652e1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved exploratory analysis complete; figures saved to ../figures/exploratory\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "INPUT_CSV = \"../data/clean/wdi_long_clean.csv\"\n",
    "FIGURES_DIR = \"../figures/exploratory\"\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Descriptive statistics table (as before)\n",
    "desc = df.groupby(['indicator_code','country_code'])['value'].describe().round(2)\n",
    "desc.to_csv(os.path.join(FIGURES_DIR, \"descriptive_stats.csv\"))\n",
    "pivot_mean = df.groupby(['country_code','indicator_code'])['value'].mean().unstack().round(2)\n",
    "pivot_mean.to_csv(os.path.join(FIGURES_DIR, \"mean_pivot.csv\"))\n",
    "\n",
    "# 1. Trend line chart: Increase size, mark, annotate last value\n",
    "for indicator in df['indicator_code'].unique():\n",
    "    sub = df[df['indicator_code']==indicator]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for country in sub['country_code'].unique():\n",
    "        csub = sub[sub['country_code']==country]\n",
    "        plt.plot(csub['year'], csub['value'], marker='o', linewidth=2, label=country)\n",
    "        # Annotate last point\n",
    "        last = csub.iloc[-1]\n",
    "        plt.text(last['year'], last['value'], f\"{last['value']:.1f}\", va='bottom', ha='right')\n",
    "    plt.title(f\"Trend of {indicator}\", fontsize=14)\n",
    "    plt.xlabel(\"Year\", fontsize=12)\n",
    "    plt.ylabel(\"Value\", fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title=\"Country\")\n",
    "    plt.grid(linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, f\"trend_{indicator}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# 2. Bar comparison for 2023: Show value on bars\n",
    "latest = df[df['year']==df['year'].max()]\n",
    "for indicator in latest['indicator_code'].unique():\n",
    "    sub = latest[latest['indicator_code']==indicator]\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    bars = plt.bar(sub['country_code'], sub['value'])\n",
    "    plt.title(f\"{indicator} in {int(df['year'].max())}\", fontsize=14)\n",
    "    plt.xlabel(\"Country\", fontsize=12)\n",
    "    plt.ylabel(\"Value\", fontsize=12)\n",
    "    # Annotate value at bar top\n",
    "    for bar in bars:\n",
    "        h = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, h, f\"{h:.1f}\", ha='center', va='bottom')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, f\"bar_{indicator}_2023.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# 3. Correlation heatmap: Use pcolormesh and add annotations\n",
    "for country in df['country_code'].unique():\n",
    "    sub = df[df['country_code']==country]\n",
    "    wide = sub.pivot(index='year', columns='indicator_code', values='value')\n",
    "    corr = wide.corr()\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    mesh = plt.pcolormesh(corr.values, edgecolors='k', linewidth=0.5)\n",
    "    plt.xticks(range(len(corr)), corr.columns, rotation=45)\n",
    "    plt.yticks(range(len(corr)), corr.index)\n",
    "    plt.title(f\"Indicator Correlation ({country})\", fontsize=14)\n",
    "    # Annotate each cell\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(corr.shape[1]):\n",
    "            plt.text(j + 0.5, i + 0.5, f\"{corr.iloc[i,j]:.2f}\", ha='center', va='center')\n",
    "    plt.colorbar(mesh)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, f\"corr_{country}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# 4. Histogram: Overall distribution of indicators\n",
    "for indicator in df['indicator_code'].unique():\n",
    "    sub = df[df['indicator_code']==indicator]\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(sub['value'], bins=15)\n",
    "    plt.title(f\"Distribution of {indicator}\", fontsize=14)\n",
    "    plt.xlabel(\"Value\", fontsize=12)\n",
    "    plt.ylabel(\"Frequency\", fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, f\"hist_{indicator}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# 5. Boxplot: Compare indicator distribution by country\n",
    "for indicator in df['indicator_code'].unique():\n",
    "    sub = df[df['indicator_code']==indicator]\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    data = [sub[sub['country_code']==c]['value'] for c in sub['country_code'].unique()]\n",
    "    plt.boxplot(data, labels=sub['country_code'].unique())\n",
    "    plt.title(f\"Boxplot of {indicator} by Country\", fontsize=14)\n",
    "    plt.xlabel(\"Country\", fontsize=12)\n",
    "    plt.ylabel(\"Value\", fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, f\"box_{indicator}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# 6. Trend smoothing: Moving average (window=3)\n",
    "for indicator in df['indicator_code'].unique():\n",
    "    sub = df[df['indicator_code']==indicator]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for country in sub['country_code'].unique():\n",
    "        csub = sub[sub['country_code']==country].set_index('year')\n",
    "        smooth = csub['value'].rolling(window=3, center=True).mean()\n",
    "        plt.plot(smooth.index, smooth.values, linewidth=2, label=country)\n",
    "    plt.title(f\"3-Year Moving Avg of {indicator}\", fontsize=14)\n",
    "    plt.xlabel(\"Year\", fontsize=12)\n",
    "    plt.ylabel(\"Smoothed Value\", fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title=\"Country\")\n",
    "    plt.grid(linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, f\"smooth_{indicator}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "print(\"Improved exploratory analysis complete; figures saved to\", FIGURES_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96f40b8",
   "metadata": {},
   "source": [
    "# Modeling and in-depth mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "571f272b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Data overview:\n",
      "- Number of rows: 216\n",
      "- Number of columns: 6\n",
      "- Country list: CHN, IND, USA\n",
      "- Indicator list: NY.GDP.MKTP.KD.ZG, NY.GDP.PCAP.KD, SL.EMP.TOTL.SP.ZS\n",
      "- Year range: 2000 - 2023\n",
      "\n",
      "Wide-format data sample:\n",
      "indicator_code country_code country_name  year  NY_GDP_MKTP_KD_ZG  \\\n",
      "0                       CHN          CHN  2000           8.490093   \n",
      "1                       CHN          CHN  2001           8.335733   \n",
      "2                       CHN          CHN  2002           9.133631   \n",
      "3                       CHN          CHN  2003          10.038030   \n",
      "4                       CHN          CHN  2004          10.113621   \n",
      "\n",
      "indicator_code  NY_GDP_PCAP_KD  SL_EMP_TOTL_SP_ZS NY_GDP_MKTP_KD_ZG_name  \\\n",
      "0                  2193.896866             71.771      NY.GDP.MKTP.KD.ZG   \n",
      "1                  2359.572385             71.070      NY.GDP.MKTP.KD.ZG   \n",
      "2                  2557.891612             70.444      NY.GDP.MKTP.KD.ZG   \n",
      "3                  2797.176659             69.891      NY.GDP.MKTP.KD.ZG   \n",
      "4                  3061.833173             69.652      NY.GDP.MKTP.KD.ZG   \n",
      "\n",
      "indicator_code NY_GDP_PCAP_KD_name SL_EMP_TOTL_SP_ZS_name  \n",
      "0                   NY.GDP.PCAP.KD      SL.EMP.TOTL.SP.ZS  \n",
      "1                   NY.GDP.PCAP.KD      SL.EMP.TOTL.SP.ZS  \n",
      "2                   NY.GDP.PCAP.KD      SL.EMP.TOTL.SP.ZS  \n",
      "3                   NY.GDP.PCAP.KD      SL.EMP.TOTL.SP.ZS  \n",
      "4                   NY.GDP.PCAP.KD      SL.EMP.TOTL.SP.ZS  \n",
      "\n",
      "\n",
      "1. Performing linear regression analysis...\n",
      "Regression NY_GDP_MKTP_KD_ZG ~ NY_GDP_PCAP_KD: Coefficient = -0.0001, p-value = 0.0000, R² = 0.4245\n",
      "Regression NY_GDP_MKTP_KD_ZG ~ SL_EMP_TOTL_SP_ZS: Coefficient = 0.1532, p-value = 0.0128, R² = 0.0853\n",
      "Regression NY_GDP_PCAP_KD ~ NY_GDP_MKTP_KD_ZG: Coefficient = -4462.6300, p-value = 0.0000, R² = 0.4245\n",
      "Regression NY_GDP_PCAP_KD ~ SL_EMP_TOTL_SP_ZS: Coefficient = 374.2393, p-value = 0.3838, R² = 0.0109\n",
      "Regression SL_EMP_TOTL_SP_ZS ~ NY_GDP_MKTP_KD_ZG: Coefficient = 0.5570, p-value = 0.0128, R² = 0.0853\n",
      "Regression SL_EMP_TOTL_SP_ZS ~ NY_GDP_PCAP_KD: Coefficient = 0.0000, p-value = 0.3838, R² = 0.0109\n",
      "\n",
      "\n",
      "2. Performing panel data analysis...\n",
      "\n",
      "Target variable: NY_GDP_MKTP_KD_ZG (NY.GDP.MKTP.KD.ZG)\n",
      "Basic model R²: 0.5555\n",
      "Fixed effects model R²: 0.5795\n",
      "Time trend model R²: 0.5959\n",
      "\n",
      "Target variable: NY_GDP_PCAP_KD (NY.GDP.PCAP.KD)\n",
      "Basic model R²: 0.5194\n",
      "Fixed effects model R²: 0.9869\n",
      "Time trend model R²: 0.9952\n",
      "\n",
      "Target variable: SL_EMP_TOTL_SP_ZS (SL.EMP.TOTL.SP.ZS)\n",
      "Basic model R²: 0.2361\n",
      "Fixed effects model R²: 0.9249\n",
      "Time trend model R²: 0.9731\n",
      "\n",
      "\n",
      "3. Performing time series analysis...\n",
      "Completed ARIMA analysis for CHN - NY.GDP.MKTP.KD.ZG, best model: ARIMA(1, 1, 0)\n",
      "Completed ARIMA analysis for CHN - NY.GDP.PCAP.KD, best model: ARIMA(0, 2, 2)\n",
      "Completed ARIMA analysis for CHN - SL.EMP.TOTL.SP.ZS, best model: ARIMA(1, 1, 0)\n",
      "Completed ARIMA analysis for IND - NY.GDP.MKTP.KD.ZG, best model: ARIMA(0, 0, 0)\n",
      "Completed ARIMA analysis for IND - NY.GDP.PCAP.KD, best model: ARIMA(0, 1, 0)\n",
      "Completed ARIMA analysis for IND - SL.EMP.TOTL.SP.ZS, best model: ARIMA(1, 1, 0)\n",
      "Completed ARIMA analysis for USA - NY.GDP.MKTP.KD.ZG, best model: ARIMA(0, 0, 0)\n",
      "Completed ARIMA analysis for USA - NY.GDP.PCAP.KD, best model: ARIMA(0, 1, 0)\n",
      "Completed ARIMA analysis for USA - SL.EMP.TOTL.SP.ZS, best model: ARIMA(0, 1, 0)\n",
      "\n",
      "\n",
      "4. Training machine learning models...\n",
      "Machine learning dataset size: 72 rows, 9 columns\n",
      "Before missing value processing: 72 rows -> After processing: 72 rows\n",
      "\n",
      "Target variable: NY_GDP_MKTP_KD_ZG (NY.GDP.MKTP.KD.ZG)\n",
      "Linear Regression: RMSE = 2.2841, R² = 0.5834\n",
      "Elastic Net: RMSE = 2.2806, R² = 0.5847\n",
      "Random Forest: RMSE = 1.8244, R² = 0.7342\n",
      "\n",
      "Target variable: NY_GDP_PCAP_KD (NY.GDP.PCAP.KD)\n",
      "Linear Regression: RMSE = 12536.9699, R² = 0.6662\n",
      "Elastic Net: RMSE = 12532.1433, R² = 0.6665\n",
      "Random Forest: RMSE = 6507.6562, R² = 0.9101\n",
      "\n",
      "Target variable: SL_EMP_TOTL_SP_ZS (SL.EMP.TOTL.SP.ZS)\n",
      "Linear Regression: RMSE = 6.8797, R² = -0.0167\n",
      "Elastic Net: RMSE = 6.8801, R² = -0.0168\n",
      "Random Forest: RMSE = 4.5900, R² = 0.5475\n",
      "\n",
      "\n",
      "5. Performing clustering analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"c:\\Users\\zihan\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "  File \"c:\\Users\\zihan\\anaconda3\\envs\\pytorch_gpu\\lib\\subprocess.py\", line 503, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"c:\\Users\\zihan\\anaconda3\\envs\\pytorch_gpu\\lib\\subprocess.py\", line 971, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\zihan\\anaconda3\\envs\\pytorch_gpu\\lib\\subprocess.py\", line 1456, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended optimal number of clusters: 2\n",
      "Completed clustering analysis for 2023\n",
      "\n",
      "Modeling and In-depth Mining Completed!\n",
      "- Charts: ../figures/modeling/\n",
      "- Model Results: ../models/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set matplotlib Chinese support\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans', 'Bitstream Vera Sans', 'sans-serif']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Configuration\n",
    "INPUT_CSV = \"../data/clean/wdi_long_clean.csv\"\n",
    "MODELS_DIR = \"../models\"\n",
    "FIGURES_DIR = \"../figures/modeling\"\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Data overview\n",
    "print(\"\\nData overview:\")\n",
    "print(f\"- Number of rows: {df.shape[0]}\")\n",
    "print(f\"- Number of columns: {df.shape[1]}\")\n",
    "print(f\"- Country list: {', '.join(df['country_code'].unique())}\")\n",
    "print(f\"- Indicator list: {', '.join(df['indicator_code'].unique())}\")\n",
    "print(f\"- Year range: {df['year'].min()} - {df['year'].max()}\")\n",
    "\n",
    "# Create wide-format data (each row is a country-year combination, columns are different indicators)\n",
    "def create_wide_df():\n",
    "    \"\"\"Create wide-format data, expanding indicators into columns\"\"\"\n",
    "    # Create a data copy for analysis\n",
    "    wide_df = df.pivot_table(\n",
    "        index=['country_code', 'country_name', 'year'],\n",
    "        columns='indicator_code',\n",
    "        values='value'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Rename columns containing dots to avoid errors in formulas\n",
    "    for col in wide_df.columns:\n",
    "        if isinstance(col, str) and '.' in col:\n",
    "            # Replace dots in indicator codes with underscores\n",
    "            new_col = col.replace('.', '_')\n",
    "            wide_df = wide_df.rename(columns={col: new_col})\n",
    "    \n",
    "    # Create a mapping from indicator code to name\n",
    "    code_to_name = {}\n",
    "    for _, row in df.drop_duplicates(['indicator_code', 'indicator_name']).iterrows():\n",
    "        code = row['indicator_code'].replace('.', '_')\n",
    "        code_to_name[code] = row['indicator_name']\n",
    "    \n",
    "    # Add indicator name columns\n",
    "    for code, name in code_to_name.items():\n",
    "        if code in wide_df.columns:\n",
    "            wide_df[f\"{code}_name\"] = name\n",
    "    \n",
    "    return wide_df, code_to_name\n",
    "\n",
    "wide_df, code_to_name = create_wide_df()\n",
    "print(\"\\nWide-format data sample:\")\n",
    "print(wide_df.head())\n",
    "\n",
    "# Get list of indicator codes\n",
    "indicator_codes = [col for col in wide_df.columns if col in [c.replace('.', '_') for c in df['indicator_code'].unique()]]\n",
    "\n",
    "# Save wide-format data\n",
    "wide_df.to_csv(os.path.join(MODELS_DIR, \"wide_format_data.csv\"), index=False)\n",
    "\n",
    "#####################################################\n",
    "# 1. Linear Regression Analysis - Explore Indicator Relationships\n",
    "#####################################################\n",
    "print(\"\\n\\n1. Performing linear regression analysis...\")\n",
    "\n",
    "def run_linear_regression(X_col, y_col, data, country=None):\n",
    "    \"\"\"Run OLS linear regression and return results\n",
    "    \n",
    "    Args:\n",
    "        X_col: Independent variable column name\n",
    "        y_col: Dependent variable column name\n",
    "        data: DataFrame\n",
    "        country: If specified, analyze only data for this country\n",
    "    \n",
    "    Returns:\n",
    "        Regression result object\n",
    "    \"\"\"\n",
    "    if country:\n",
    "        data = data[data['country_code'] == country]\n",
    "    \n",
    "    # Prepare data\n",
    "    X = data[[X_col]].values\n",
    "    y = data[y_col].values\n",
    "    \n",
    "    # Add constant term\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    # Fit model\n",
    "    model = sm.OLS(y, X)\n",
    "    result = model.fit()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Analyze relationships between all indicator pairs\n",
    "regression_results = {}\n",
    "\n",
    "# Create all possible indicator pair combinations\n",
    "for i, target in enumerate(indicator_codes):\n",
    "    for j, predictor in enumerate(indicator_codes):\n",
    "        if i != j:  # Avoid predicting a variable from itself\n",
    "            key = f\"{target} ~ {predictor}\"\n",
    "            result = run_linear_regression(predictor, target, wide_df)\n",
    "            \n",
    "            # Get coefficients (index 1 because index 0 corresponds to constant term)\n",
    "            coef = result.params[1]\n",
    "            p_val = result.pvalues[1]\n",
    "            \n",
    "            regression_results[key] = {\n",
    "                'coefficient': coef,\n",
    "                'p_value': p_val,\n",
    "                'r_squared': result.rsquared,\n",
    "                'result': result\n",
    "            }\n",
    "            print(f\"Regression {key}: Coefficient = {coef:.4f}, p-value = {p_val:.4f}, R² = {result.rsquared:.4f}\")\n",
    "\n",
    "# Visualize regression lines for each indicator pair\n",
    "for key, res in regression_results.items():\n",
    "    target, predictor = key.split(' ~ ')\n",
    "    result = res['result']\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.regplot(x=predictor, y=target, data=wide_df, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "    \n",
    "    # Get indicator names\n",
    "    target_name = code_to_name.get(target, target)\n",
    "    predictor_name = code_to_name.get(predictor, predictor)\n",
    "    \n",
    "    # Add confidence interval\n",
    "    x_sorted = wide_df[predictor].sort_values()\n",
    "    X_sorted = sm.add_constant(x_sorted)\n",
    "    y_pred = result.predict(X_sorted)\n",
    "    \n",
    "    # Calculate prediction confidence interval\n",
    "    pred_ci = result.get_prediction(X_sorted).conf_int(alpha=0.05)\n",
    "    \n",
    "    plt.fill_between(\n",
    "        x_sorted,\n",
    "        pred_ci[:, 0],\n",
    "        pred_ci[:, 1],\n",
    "        alpha=0.2, color='red'\n",
    "    )\n",
    "    \n",
    "    # Add annotations\n",
    "    plt.title(f\"Relationship between {target_name} and {predictor_name}\", fontsize=14)\n",
    "    plt.xlabel(predictor_name, fontsize=12)\n",
    "    plt.ylabel(target_name, fontsize=12)\n",
    "    \n",
    "    # Add regression equation and R²\n",
    "    eq_text = f\"y = {result.params[1]:.4f}x + {result.params[0]:.4f}\"\n",
    "    r2_text = f\"R² = {result.rsquared:.4f}\"\n",
    "    p_text = f\"p-value = {result.pvalues[1]:.4f}\"\n",
    "    \n",
    "    plt.annotate(eq_text + '\\n' + r2_text + '\\n' + p_text, \n",
    "                xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "                bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", alpha=0.8),\n",
    "                va='top')\n",
    "    \n",
    "    plt.grid(linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, f\"regression_{target}_vs_{predictor}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Save main regression results\n",
    "pd.DataFrame([\n",
    "    {\n",
    "        'target': k.split(' ~ ')[0],\n",
    "        'predictor': k.split(' ~ ')[1],\n",
    "        'coefficient': v['coefficient'],\n",
    "        'p_value': v['p_value'],\n",
    "        'r_squared': v['r_squared']\n",
    "    }\n",
    "    for k, v in regression_results.items()\n",
    "]).to_csv(os.path.join(MODELS_DIR, \"regression_results.csv\"), index=False)\n",
    "\n",
    "#####################################################\n",
    "# 2. Panel Data Analysis - Control for Country and Time Fixed Effects\n",
    "#####################################################\n",
    "print(\"\\n\\n2. Performing panel data analysis...\")\n",
    "\n",
    "# Create data for panel analysis (add country and year dummy variables)\n",
    "panel_df = wide_df.copy()\n",
    "\n",
    "# Convert countries to dummy variables\n",
    "panel_df = pd.get_dummies(panel_df, columns=['country_code'], drop_first=True)\n",
    "\n",
    "# For each indicator, perform panel regression analysis\n",
    "panel_results = {}\n",
    "\n",
    "for target in indicator_codes:\n",
    "    predictors = [ind for ind in indicator_codes if ind != target]\n",
    "    \n",
    "    # Prepare dataset\n",
    "    y = panel_df[target]\n",
    "    \n",
    "    # Basic model (without fixed effects)\n",
    "    X_basic = panel_df[predictors].copy()\n",
    "    X_basic = sm.add_constant(X_basic)\n",
    "    basic_model = sm.OLS(y, X_basic)\n",
    "    basic_result = basic_model.fit()\n",
    "    \n",
    "    # Fixed effects model (add country dummy variables)\n",
    "    country_dummies = [col for col in panel_df.columns if col.startswith('country_code_')]\n",
    "    X_fe = panel_df[predictors + country_dummies].copy()\n",
    "    for col in X_fe.columns:\n",
    "        X_fe[col] = pd.to_numeric(X_fe[col], errors='coerce')\n",
    "    y = pd.to_numeric(panel_df[target], errors='coerce')\n",
    "\n",
    "    df_model = pd.DataFrame({'y': y}).join(X_fe)\n",
    "    df_model = df_model.dropna()\n",
    "    y_clean = df_model['y']\n",
    "    X_clean = df_model.drop(columns=['y'])\n",
    "\n",
    "    X_clean = sm.add_constant(X_clean)\n",
    "    X_clean = X_clean.astype(float)\n",
    "    y_clean = y_clean.astype(float)\n",
    "\n",
    "    fe_result = sm.OLS(y_clean, X_clean).fit()\n",
    "    \n",
    "    # Time trend model (add year as a linear trend)\n",
    "    X_trend = panel_df[predictors + country_dummies + ['year']].copy()\n",
    "    X_trend = sm.add_constant(X_trend)\n",
    "\n",
    "    X_trend = X_trend.astype(float)\n",
    "    y_trend = pd.to_numeric(panel_df[target], errors='coerce').astype(float)\n",
    "\n",
    "    df_trend = pd.concat([y_trend.rename(target), X_trend], axis=1).dropna()\n",
    "    y_trend_clean = df_trend[target]\n",
    "    X_trend_clean = df_trend.drop(columns=[target])\n",
    "\n",
    "    trend_result = sm.OLS(y_trend_clean, X_trend_clean).fit()\n",
    "    \n",
    "    panel_results[target] = {\n",
    "        'basic': basic_result,\n",
    "        'fixed_effects': fe_result,\n",
    "        'time_trend': trend_result\n",
    "    }\n",
    "    \n",
    "    # Print results summary\n",
    "    target_name = code_to_name.get(target, target)\n",
    "    print(f\"\\nTarget variable: {target} ({target_name})\")\n",
    "    print(f\"Basic model R²: {basic_result.rsquared:.4f}\")\n",
    "    print(f\"Fixed effects model R²: {fe_result.rsquared:.4f}\")\n",
    "    print(f\"Time trend model R²: {trend_result.rsquared:.4f}\")\n",
    "\n",
    "# Save panel regression results\n",
    "for target, models in panel_results.items():\n",
    "    target_name = code_to_name.get(target, target)\n",
    "    \n",
    "    # Create a comparison table\n",
    "    coef_table = pd.DataFrame()\n",
    "    \n",
    "    # For each predictor, extract coefficients and p-values\n",
    "    predictors = [ind for ind in indicator_codes if ind != target]\n",
    "    \n",
    "    for predictor in predictors:\n",
    "        predictor_name = code_to_name.get(predictor, predictor)\n",
    "        \n",
    "        # Extract coefficients and p-values by predictor name\n",
    "        coef_basic = models['basic'].params.get(predictor, np.nan)\n",
    "        p_basic    = models['basic'].pvalues.get(predictor, np.nan)\n",
    "\n",
    "        coef_fe    = models['fixed_effects'].params.get(predictor, np.nan)\n",
    "        p_fe       = models['fixed_effects'].pvalues.get(predictor, np.nan)\n",
    "\n",
    "        coef_trend = models['time_trend'].params.get(predictor, np.nan)\n",
    "        p_trend    = models['time_trend'].pvalues.get(predictor, np.nan)\n",
    "        \n",
    "        # Add to table\n",
    "        row = {\n",
    "            'Predictor': predictor_name,\n",
    "            'Basic Model_Coefficient': coef_basic,\n",
    "            'Basic Model_p-value': p_basic,\n",
    "            'Fixed Effects_Coefficient': coef_fe,\n",
    "            'Fixed Effects_p-value': p_fe,\n",
    "            'Time Trend_Coefficient': coef_trend,\n",
    "            'Time Trend_p-value': p_trend\n",
    "        }\n",
    "        coef_table = pd.concat([coef_table, pd.DataFrame([row])], ignore_index=True)\n",
    "    \n",
    "    # Add model information\n",
    "    info_row = {\n",
    "        'Predictor': 'Model Information',\n",
    "        'Basic Model_Coefficient': f\"R²: {models['basic'].rsquared:.4f}\",\n",
    "        'Basic Model_p-value': f\"Adj R²: {models['basic'].rsquared_adj:.4f}\",\n",
    "        'Fixed Effects_Coefficient': f\"R²: {models['fixed_effects'].rsquared:.4f}\",\n",
    "        'Fixed Effects_p-value': f\"Adj R²: {models['fixed_effects'].rsquared_adj:.4f}\",\n",
    "        'Time Trend_Coefficient': f\"R²: {models['time_trend'].rsquared:.4f}\",\n",
    "        'Time Trend_p-value': f\"Adj R²: {models['time_trend'].rsquared_adj:.4f}\"\n",
    "    }\n",
    "    coef_table = pd.concat([coef_table, pd.DataFrame([info_row])], ignore_index=True)\n",
    "    \n",
    "    # Save table\n",
    "    coef_table.to_csv(os.path.join(MODELS_DIR, f\"panel_results_{target}.csv\"), index=False)\n",
    "    \n",
    "    # Visualize model comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot original data\n",
    "    for country in df['country_code'].unique():\n",
    "        country_data = wide_df[wide_df['country_code'] == country]\n",
    "        plt.scatter(country_data['year'], country_data[target], alpha=0.7, label=country if len(df['country_code'].unique()) <= 5 else None)\n",
    "    \n",
    "    # Plot different model predictions\n",
    "    sorted_data = panel_df.sort_values(['year'])\n",
    "    \n",
    "    # Predict basic model\n",
    "    y_pred_basic = models['basic'].predict(X_basic)\n",
    "    \n",
    "    # Plot basic model prediction\n",
    "    plt.plot(sorted_data['year'], y_pred_basic, 'r-', linewidth=2, label='Basic Model')\n",
    "    \n",
    "    plt.title(f\"{target_name} - Panel Data Models\", fontsize=14)\n",
    "    plt.xlabel(\"Year\", fontsize=12)\n",
    "    plt.ylabel(target_name, fontsize=12)\n",
    "    plt.legend(title=\"Model/Country\")\n",
    "    plt.grid(linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, f\"panel_models_{target}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "#####################################################\n",
    "# 3. Time Series Analysis - ARIMA Model for Trend Prediction\n",
    "#####################################################\n",
    "print(\"\\n\\n3. Performing time series analysis...\")\n",
    "\n",
    "def fit_arima_model(series, country_name, indicator_name):\n",
    "    \"\"\"Fit ARIMA model and return results\"\"\"\n",
    "    \n",
    "    # Check number of data points\n",
    "    if len(series) < 8:  # At least 8 data points needed\n",
    "        print(f\"Warning: Insufficient data points for {country_name}'s {indicator_name}, skipping ARIMA analysis\")\n",
    "        return None\n",
    "    \n",
    "    # Use auto_arima to automatically find best parameters\n",
    "    auto_model = auto_arima(\n",
    "        series,\n",
    "        start_p=0, start_q=0,\n",
    "        max_p=3, max_q=3, max_d=2,\n",
    "        seasonal=False,  # Assume no seasonality\n",
    "        trace=False,\n",
    "        error_action='ignore',\n",
    "        suppress_warnings=True,\n",
    "        stepwise=True\n",
    "    )\n",
    "    \n",
    "    # Get best parameters\n",
    "    best_order = auto_model.order\n",
    "    \n",
    "    # Build ARIMA model with best parameters\n",
    "    model = ARIMA(series, order=best_order)\n",
    "    result = model.fit()\n",
    "    \n",
    "    return {\n",
    "        'model': result,\n",
    "        'order': best_order,\n",
    "        'aic': result.aic,\n",
    "        'bic': result.bic\n",
    "    }\n",
    "\n",
    "# Fit ARIMA models for each country and indicator combination\n",
    "arima_results = {}\n",
    "forecast_years = 5  # Forecast next 5 years\n",
    "\n",
    "for country in df['country_code'].unique():\n",
    "    arima_results[country] = {}\n",
    "    \n",
    "    for indicator_code in df['indicator_code'].unique():\n",
    "        # Replace dots with underscores in indicator code\n",
    "        indicator = indicator_code.replace('.', '_')\n",
    "        \n",
    "        # Extract time series for this country and indicator\n",
    "        country_data = df[(df['country_code'] == country) & (df['indicator_code'] == indicator_code)]\n",
    "        country_data = country_data.sort_values('year')\n",
    "        \n",
    "        if country_data.empty:\n",
    "            continue\n",
    "            \n",
    "        # Get indicator name\n",
    "        indicator_name = country_data['indicator_name'].iloc[0]\n",
    "        \n",
    "        # Set time series\n",
    "        ts = pd.Series(country_data['value'].values, index=pd.to_datetime(country_data['year'], format='%Y'))\n",
    "        \n",
    "        # Fit ARIMA model\n",
    "        result = fit_arima_model(ts, country, indicator_name)\n",
    "        \n",
    "        if result is None:\n",
    "            continue\n",
    "            \n",
    "        arima_results[country][indicator] = result\n",
    "        model = result['model']\n",
    "        \n",
    "        # Forecast next few years\n",
    "        last_year = country_data['year'].max()\n",
    "        future_years = pd.date_range(start=f\"{last_year+1}-01-01\", periods=forecast_years, freq='YS')\n",
    "        \n",
    "        forecast = model.forecast(steps=forecast_years)\n",
    "        forecast_df = pd.DataFrame({\n",
    "            'year': [y.year for y in future_years],\n",
    "            'forecast': forecast,\n",
    "            'lower_ci': forecast - 1.96 * model.resid.std(),\n",
    "            'upper_ci': forecast + 1.96 * model.resid.std(),\n",
    "        })\n",
    "        \n",
    "        # Save forecast results\n",
    "        forecast_df.to_csv(os.path.join(MODELS_DIR, f\"forecast_{country}_{indicator}.csv\"), index=False)\n",
    "        \n",
    "        # Visualize fitting and forecasting\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Plot original data\n",
    "        plt.plot(country_data['year'], country_data['value'], 'o-', label='Historical Data')\n",
    "        \n",
    "        # Plot fitted values\n",
    "        in_sample_pred = model.fittedvalues\n",
    "        plt.plot(ts.index.year, in_sample_pred, 'r--', label='Model Fit')\n",
    "        \n",
    "        # Plot forecast and confidence interval\n",
    "        plt.plot(forecast_df['year'], forecast_df['forecast'], 'g-', label='Forecast')\n",
    "        plt.fill_between(\n",
    "            forecast_df['year'],\n",
    "            forecast_df['lower_ci'],\n",
    "            forecast_df['upper_ci'],\n",
    "            color='g', alpha=0.2, label='95% Confidence Interval'\n",
    "        )\n",
    "        \n",
    "        plt.title(f\"{country} - {indicator_name} Time Series Model and Forecast\", fontsize=14)\n",
    "        plt.xlabel(\"Year\", fontsize=12)\n",
    "        plt.ylabel(indicator_name, fontsize=12)\n",
    "        \n",
    "        # Add model parameter annotations\n",
    "        order = result['order']\n",
    "        aic = result['aic']\n",
    "        order_text = f\"ARIMA{order}\"\n",
    "        aic_text = f\"AIC: {aic:.2f}\"\n",
    "        \n",
    "        plt.annotate(\n",
    "            order_text + '\\n' + aic_text,\n",
    "            xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", alpha=0.8),\n",
    "            va='top'\n",
    "        )\n",
    "        \n",
    "        plt.grid(linestyle='--', alpha=0.7)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, f\"arima_{country}_{indicator}.png\"))\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Completed ARIMA analysis for {country} - {indicator_name}, best model: ARIMA{order}\")\n",
    "\n",
    "# Visualize ACF and PACF plots\n",
    "for country in df['country_code'].unique():\n",
    "    for indicator_code in df['indicator_code'].unique():\n",
    "        country_data = df[(df['country_code'] == country) & (df['indicator_code'] == indicator_code)]\n",
    "        country_data = country_data.sort_values('year')\n",
    "        \n",
    "        if country_data.empty or len(country_data) < 8:\n",
    "            continue\n",
    "            \n",
    "        indicator_name = country_data['indicator_name'].iloc[0]\n",
    "        indicator = indicator_code.replace('.', '_')\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "        \n",
    "        # ACF\n",
    "        plot_acf(country_data['value'], lags=min(10, len(country_data)-1), ax=ax1, title=f\"{country} - {indicator_name} Autocorrelation Function\")\n",
    "        \n",
    "        # PACF\n",
    "        plot_pacf(country_data['value'], lags=min(10, len(country_data)-1), ax=ax2, title=f\"{country} - {indicator_name} Partial Autocorrelation Function\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, f\"acf_pacf_{country}_{indicator}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "#####################################################\n",
    "# 4. Machine Learning Models - Random Forest Prediction and Feature Importance Analysis\n",
    "#####################################################\n",
    "print(\"\\n\\n4. Training machine learning models...\")\n",
    "\n",
    "# Prepare machine learning dataset\n",
    "ml_df = wide_df.dropna()  # Remove missing values\n",
    "\n",
    "# Save data preparation results\n",
    "print(f\"Machine learning dataset size: {ml_df.shape[0]} rows, {ml_df.shape[1]} columns\")\n",
    "print(f\"Before missing value processing: {wide_df.shape[0]} rows -> After processing: {ml_df.shape[0]} rows\")\n",
    "\n",
    "# Analyze each indicator\n",
    "for target in indicator_codes:\n",
    "    target_name = code_to_name.get(target, target)\n",
    "    \n",
    "    # Select features and target\n",
    "    features = [ind for ind in indicator_codes if ind != target]\n",
    "    X = ml_df[features]\n",
    "    y = ml_df[target]\n",
    "    \n",
    "    # Split train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "    \n",
    "    # 1. Linear Regression\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    lr_pred = lr.predict(X_test)\n",
    "    lr_rmse = np.sqrt(mean_squared_error(y_test, lr_pred))\n",
    "    lr_r2 = r2_score(y_test, lr_pred)\n",
    "    \n",
    "    # 2. Elastic Net\n",
    "    en = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
    "    en.fit(X_train, y_train)\n",
    "    en_pred = en.predict(X_test)\n",
    "    en_rmse = np.sqrt(mean_squared_error(y_test, en_pred))\n",
    "    en_r2 = r2_score(y_test, en_pred)\n",
    "    \n",
    "    # 3. Random Forest\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_pred = rf.predict(X_test)\n",
    "    rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "    rf_r2 = r2_score(y_test, rf_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nTarget variable: {target} ({target_name})\")\n",
    "    print(f\"Linear Regression: RMSE = {lr_rmse:.4f}, R² = {lr_r2:.4f}\")\n",
    "    print(f\"Elastic Net: RMSE = {en_rmse:.4f}, R² = {en_r2:.4f}\")\n",
    "    print(f\"Random Forest: RMSE = {rf_rmse:.4f}, R² = {rf_r2:.4f}\")\n",
    "    \n",
    "    # Save results\n",
    "    models_comparison = pd.DataFrame({\n",
    "        'Model': ['Linear Regression', 'Elastic Net', 'Random Forest'],\n",
    "        'RMSE': [lr_rmse, en_rmse, rf_rmse],\n",
    "        'R²': [lr_r2, en_r2, rf_r2]\n",
    "    })\n",
    "    models_comparison.to_csv(os.path.join(MODELS_DIR, f\"ml_models_comparison_{target}.csv\"), index=False)\n",
    "    \n",
    "    # Visualize predictions vs actual\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Scatter plot: Actual vs Predicted\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(y_test, lr_pred, alpha=0.7)\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "    plt.title('Linear Regression: Predicted vs Actual')\n",
    "    plt.xlabel('Actual Value')\n",
    "    plt.ylabel('Predicted Value')\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(y_test, en_pred, alpha=0.7)\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "    plt.title('Elastic Net: Predicted vs Actual')\n",
    "    plt.xlabel('Actual Value')\n",
    "    plt.ylabel('Predicted Value')\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.scatter(y_test, rf_pred, alpha=0.7)\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "    plt.title('Random Forest: Predicted vs Actual')\n",
    "    plt.xlabel('Actual Value')\n",
    "    plt.ylabel('Predicted Value')\n",
    "    \n",
    "    # Bar plot: Model performance comparison\n",
    "    plt.subplot(2, 2, 4)\n",
    "    models = ['Linear Regression', 'Elastic Net', 'Random Forest']\n",
    "    r2_values = [lr_r2, en_r2, rf_r2]\n",
    "    rmse_values = [lr_rmse, en_rmse, rf_rmse]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, r2_values, width, label='R²')\n",
    "    plt.bar(x + width/2, rmse_values, width, label='RMSE')\n",
    "    \n",
    "    plt.xlabel('Model')\n",
    "    plt.xticks(x, models, rotation=45)\n",
    "    plt.title('Model Performance Comparison')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, f\"ml_models_pred_vs_actual_{target}.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Linear regression coefficients\n",
    "    plt.subplot(1, 2, 1)\n",
    "    coefs = pd.DataFrame({\n",
    "        'Features': features,\n",
    "        'Coefficients': lr.coef_\n",
    "    })\n",
    "    coefs = coefs.reindex(coefs['Coefficients'].abs().sort_values(ascending=False).index)\n",
    "    \n",
    "    feature_names = []\n",
    "    for feat in coefs['Features']:\n",
    "        feat_name = code_to_name.get(feat, feat)\n",
    "        feature_names.append(feat_name)\n",
    "    \n",
    "    coefs['Feature Names'] = feature_names\n",
    "    \n",
    "    sns.barplot(x='Coefficients', y='Feature Names', data=coefs)\n",
    "    plt.title('Linear Regression Coefficients')\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.ylabel('Features')\n",
    "    \n",
    "    # Random Forest feature importance\n",
    "    plt.subplot(1, 2, 2)\n",
    "    importances = pd.DataFrame({\n",
    "        'Features': features,\n",
    "        'Importance': rf.feature_importances_\n",
    "    })\n",
    "    importances = importances.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    feature_names = []\n",
    "    for feat in importances['Features']:\n",
    "        feat_name = code_to_name.get(feat, feat)\n",
    "        feature_names.append(feat_name)\n",
    "    \n",
    "    importances['Feature Names'] = feature_names\n",
    "    \n",
    "    sns.barplot(x='Importance', y='Feature Names', data=importances)\n",
    "    plt.title('Random Forest Feature Importance')\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, f\"feature_importance_{target}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "#####################################################\n",
    "# 5. Clustering Analysis - Country Grouping Based on Multiple Indicators\n",
    "#####################################################\n",
    "print(\"\\n\\n5. Performing clustering analysis...\")\n",
    "\n",
    "# Prepare clustering data\n",
    "def prepare_cluster_data(year=None):\n",
    "    if year is not None:\n",
    "        year_data = wide_df[wide_df['year'] == year].copy()\n",
    "        cluster_cols = indicator_codes\n",
    "        cluster_data = year_data[['country_code', 'country_name'] + cluster_cols].copy()\n",
    "        \n",
    "        for col in cluster_cols:\n",
    "            cluster_data[col] = pd.to_numeric(cluster_data[col], errors='coerce')\n",
    "            \n",
    "        cluster_data = cluster_data.dropna()\n",
    "        \n",
    "        cluster_data['country_name'] = cluster_data['country_name'].astype(str)\n",
    "        \n",
    "        subtitle = f\"Data from {year}\"\n",
    "    else:\n",
    "        cluster_cols = indicator_codes\n",
    "        cluster_data = (\n",
    "            wide_df\n",
    "            .loc[:, ['country_code','country_name'] + cluster_cols]\n",
    "            .groupby(['country_code', 'country_name'])[cluster_cols]\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "        )\n",
    "        \n",
    "        cluster_data['country_name'] = cluster_data['country_name'].astype(str)\n",
    "        \n",
    "        subtitle = \"Average of all years\"\n",
    "    \n",
    "    # Standardize data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(cluster_data[cluster_cols])\n",
    "    scaled_df = pd.DataFrame(scaled_data, columns=cluster_cols)\n",
    "    scaled_df['country_code'] = cluster_data['country_code']\n",
    "    scaled_df['country_name'] = cluster_data['country_name']\n",
    "    \n",
    "    return scaled_df, cluster_cols, subtitle\n",
    "\n",
    "# Determine optimal number of clusters\n",
    "def find_optimal_clusters(data, max_clusters=10):\n",
    "    \"\"\"Use elbow method to determine optimal number of clusters\"\"\"\n",
    "    n_samples = data.shape[0]\n",
    "    max_k = min(max_clusters, n_samples)\n",
    "    \n",
    "    inertia = []\n",
    "    for k in range(1, max_k + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(data)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "    return inertia\n",
    "\n",
    "# Perform clustering analysis\n",
    "def perform_clustering(scaled_df, cluster_cols, n_clusters, subtitle):\n",
    "    \"\"\"Perform K-means clustering and visualize results\"\"\"\n",
    "    # Fit K-means model\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(scaled_df[cluster_cols])\n",
    "    \n",
    "    # Add cluster labels\n",
    "    scaled_df['cluster'] = clusters\n",
    "    \n",
    "    # Calculate cluster centers\n",
    "    cluster_centers = pd.DataFrame(kmeans.cluster_centers_, columns=cluster_cols)\n",
    "    \n",
    "    # Visualize clustering results\n",
    "    \n",
    "    # 1. If more than 2 indicators, use PCA for dimensionality reduction\n",
    "    if len(cluster_cols) > 2:\n",
    "        pca = PCA(n_components=2)\n",
    "        pca_result = pca.fit_transform(scaled_df[cluster_cols])\n",
    "        pca_df = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])\n",
    "        pca_df['country_code'] = scaled_df['country_code']\n",
    "        pca_df['country_name'] = scaled_df['country_name']\n",
    "        pca_df['cluster'] = scaled_df['cluster']\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for cluster in range(n_clusters):\n",
    "            cluster_data = pca_df[pca_df['cluster'] == cluster]\n",
    "            plt.scatter(cluster_data['PC1'], cluster_data['PC2'], label=f'Cluster {cluster}')\n",
    "            \n",
    "            # Annotate country names\n",
    "            for i, txt in enumerate(cluster_data['country_code']):\n",
    "                plt.annotate(txt, (cluster_data['PC1'].iloc[i], cluster_data['PC2'].iloc[i]))\n",
    "        \n",
    "        # Visualize cluster centers\n",
    "        centers_pca = pca.transform(cluster_centers)\n",
    "        plt.scatter(centers_pca[:, 0], centers_pca[:, 1], s=200, marker='X', c='red', label='Cluster Centers')\n",
    "        \n",
    "        plt.title(f\"Country Clustering Based on World Bank Indicators\\n({subtitle}, PCA Dimensionality Reduction)\")\n",
    "        plt.xlabel(f\"Principal Component 1 (Explained Variance: {pca.explained_variance_ratio_[0]:.2f})\")\n",
    "        plt.ylabel(f\"Principal Component 2 (Explained Variance: {pca.explained_variance_ratio_[1]:.2f})\")\n",
    "        plt.legend()\n",
    "        plt.grid(linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, f\"cluster_pca_{subtitle.replace(' ', '_')}.png\"))\n",
    "        plt.close()\n",
    "        \n",
    "    # 2. If exactly 2 indicators, plot directly\n",
    "    elif len(cluster_cols) == 2:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for cluster in range(n_clusters):\n",
    "            cluster_data = scaled_df[scaled_df['cluster'] == cluster]\n",
    "            plt.scatter(cluster_data[cluster_cols[0]], cluster_data[cluster_cols[1]], label=f'Cluster {cluster}')\n",
    "            \n",
    "            # Annotate country names\n",
    "            for i, txt in enumerate(cluster_data['country_code']):\n",
    "                plt.annotate(txt, (cluster_data[cluster_cols[0]].iloc[i], cluster_data[cluster_cols[1]].iloc[i]))\n",
    "        \n",
    "        # Visualize cluster centers\n",
    "        plt.scatter(\n",
    "            cluster_centers[cluster_cols[0]], \n",
    "            cluster_centers[cluster_cols[1]], \n",
    "            s=200, marker='X', c='red', label='Cluster Centers'\n",
    "        )\n",
    "        \n",
    "        plt.title(f\"Country Clustering Based on {code_to_name.get(cluster_cols[0], cluster_cols[0])} and\\n{code_to_name.get(cluster_cols[1], cluster_cols[1])}\\n({subtitle})\")\n",
    "        plt.xlabel(code_to_name.get(cluster_cols[0], cluster_cols[0]))\n",
    "        plt.ylabel(code_to_name.get(cluster_cols[1], cluster_cols[1]))\n",
    "        plt.legend()\n",
    "        plt.grid(linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, f\"cluster_2d_{subtitle.replace(' ', '_')}.png\"))\n",
    "        plt.close()\n",
    "    \n",
    "    # 3. Heatmap: Show feature means for each cluster\n",
    "    cluster_means = scaled_df.groupby('cluster')[cluster_cols].mean()\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(cluster_means, annot=True, cmap='coolwarm', linewidths=.5)\n",
    "    plt.title(f\"Feature Means for Each Cluster ({subtitle})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, f\"cluster_heatmap_{subtitle.replace(' ', '_')}.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Table: List of countries in each cluster\n",
    "    country_clusters = scaled_df[['country_code', 'country_name', 'cluster']].sort_values(['cluster', 'country_name'])\n",
    "    cluster_summary = pd.DataFrame()\n",
    "\n",
    "    for cluster in range(n_clusters):\n",
    "        countries = country_clusters[country_clusters['cluster'] == cluster]['country_name'].tolist()\n",
    "        \n",
    "        countries = [str(c) for c in countries]\n",
    "        countries_str = ', '.join(countries)\n",
    "        \n",
    "        row = pd.DataFrame({\n",
    "            'Cluster Number': [cluster],\n",
    "            'Number of Countries': [len(countries)],\n",
    "            'Country List': [countries_str]\n",
    "        })\n",
    "        \n",
    "        cluster_summary = pd.concat([cluster_summary, row])\n",
    "    \n",
    "    cluster_summary.to_csv(os.path.join(MODELS_DIR, f\"cluster_countries_{subtitle.replace(' ', '_')}.csv\"), index=False)\n",
    "    \n",
    "    return scaled_df, cluster_centers\n",
    "\n",
    "# Analyze average of all years\n",
    "scaled_df, cluster_cols, subtitle = prepare_cluster_data()\n",
    "\n",
    "# Determine optimal number of clusters\n",
    "inertia = find_optimal_clusters(scaled_df[cluster_cols])\n",
    "\n",
    "# Visualize elbow plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(inertia) + 1), inertia, 'o-')\n",
    "plt.title('K-means Clustering Elbow Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid(linestyle='--', alpha=0.7)\n",
    "plt.savefig(os.path.join(FIGURES_DIR, \"cluster_elbow.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Determine optimal number of clusters (simple heuristic method: find inflection point)\n",
    "k_diff = np.diff(inertia)\n",
    "k_diff2 = np.diff(k_diff)\n",
    "optimal_k = np.argmin(k_diff2) + 2  # +2 because diff reduces array length, and clusters start from 1\n",
    "\n",
    "optimal_k = min(optimal_k, len(df['country_code'].unique()) - 1)  # Limit maximum clusters\n",
    "optimal_k = max(2, optimal_k)  # At least 2 clusters\n",
    "\n",
    "print(f\"Recommended optimal number of clusters: {optimal_k}\")\n",
    "\n",
    "# Perform clustering analysis\n",
    "clustered_df, cluster_centers = perform_clustering(scaled_df, cluster_cols, optimal_k, subtitle)\n",
    "\n",
    "# Perform clustering analysis for the last year\n",
    "last_year = df['year'].max()\n",
    "try:\n",
    "    last_year_df, last_year_cols, last_year_subtitle = prepare_cluster_data(year=last_year)\n",
    "    \n",
    "    if len(last_year_df) >= optimal_k and last_year_df.shape[0] > 0:\n",
    "        last_year_clustered, last_year_centers = perform_clustering(last_year_df, last_year_cols, optimal_k, last_year_subtitle)\n",
    "        print(f\"Completed clustering analysis for {last_year}\")\n",
    "    else:\n",
    "        print(f\"Not enough data for clustering in year {last_year} (found {len(last_year_df)} samples, need at least {optimal_k})\")\n",
    "except Exception as e:\n",
    "    print(f\"Error performing clustering analysis for {last_year}: {e}\")\n",
    "\n",
    "print(\"\\nModeling and In-depth Mining Completed!\")\n",
    "print(f\"- Charts: {FIGURES_DIR}/\")\n",
    "print(f\"- Model Results: {MODELS_DIR}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
